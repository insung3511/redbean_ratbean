{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "\n",
    "import GetOldTweets3 as got\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/data_train.csv\",encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Emotion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = copy.deepcopy(data)\n",
    "\n",
    "netral_data = data_[data_.Emotion=='neutral']\n",
    "sadness_data = data_[data_.Emotion=='sadness']\n",
    "fear_data = data_[data_.Emotion=='fear']\n",
    "anger_data = data_[data_.Emotion=='anger']\n",
    "joy_data = data_[data_.Emotion=='joy']\n",
    "\n",
    "sub_data = pd.concat([netral_data, sadness_data, fear_data, anger_data, joy_data], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target=data.groupby('Emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date\n",
    "\n",
    "At what time do people like to tweet? Is there a clear link between the time of tweeting and the emotion of the content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words\n",
    "\n",
    "Words distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netral_data = data_[data_.Emotion=='neutral']\n",
    "print(f\"netural data shape : {netral_data.shape}\")\n",
    "\n",
    "sadness_data = data_[data_.Emotion=='sadness']\n",
    "print(f\"sadness data shape : {sadness_data.shape}\")\n",
    "\n",
    "fear_data = data_[data_.Emotion=='fear']\n",
    "print(f\"feat data shape : {fear_data.shape}\")\n",
    "\n",
    "anger_data = data_[data_.Emotion=='anger']\n",
    "print(f\"anger data shape : {anger_data.shape}\")\n",
    "\n",
    "joy_data = data_[data_.Emotion=='joy']\n",
    "print(f\"joy data shape : {joy_data.shape}\")\n",
    "\n",
    "emo_data = pd.concat([netral_data, sadness_data, fear_data, anger_data, joy_data], axis=0)\n",
    "print(data.shape)\n",
    "\n",
    "emo_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/training.1600000.processed.noemoticon.csv\",encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"TweetText\"]\n",
    "data.columns = DATASET_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = copy.deepcopy(data)\n",
    "\n",
    "positif_data = data_[data_.target==4].iloc[:80000,:]\n",
    "netural_data = data_[data_.target==2].iloc[:80000, :]\n",
    "negative_data = data_[data_.target==0].iloc[:80000,:]\n",
    "\n",
    "sub_data = pd.concat([positif_data, netural_data, negative_data],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target=data.groupby('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date\n",
    "\n",
    "At what time do people like to tweet? Is there a clear link between the time of tweeting and the emotion of the content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = {'target': data['target'], 'date': data['date']}\n",
    "df = pd.DataFrame(data_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets ensure the 'date' column is in date format\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = [ df['date'][i].hour for i in range(len(df['date'])) ]\n",
    "df['hour'] = hour\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_data = {'0': [0]*24, '2': [0]*24, '4': [0]*24}\n",
    "for i in range(len(df['hour'])):\n",
    "    target = str(df['target'][i])\n",
    "    hour = int(df['hour'][i])\n",
    "    hour_data[target][hour] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_data = [hour_data['0'], hour_data['2'], hour_data['4']]\n",
    "# Transpose\n",
    "hour_data = list(map(list,zip(*hour_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words\n",
    "\n",
    "Words distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = copy.deepcopy(sub_data)\n",
    "newdata.drop(['ids','date','flag','user'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positif_data = data[data.target==4]\n",
    "print(positif_data.shape)\n",
    "\n",
    "negative_data = data[data.target==0]\n",
    "print(negative_data.shape)\n",
    "\n",
    "deep_data = pd.concat([positif_data,negative_data],axis = 0)\n",
    "print(data.shape)\n",
    "\n",
    "deep_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Clean_TweetText'] = data['TweetText'].str.replace(\"@\", \"\") \n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].str.replace(r\"http\\S+\", \"\") \n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].str.replace(\"[^a-zA-Z]\", \" \") \n",
    "\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    clean_text=' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda text : remove_stopwords(text.lower()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Tokenization and Normalization\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: word_tokenize(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let’s stitch these tokens back together\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: ' '.join([w for w in x]))\n",
    "# Removing small words\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english') \n",
    "emo_cv = count_vectorizer.fit_transform(emo_data['Text'])\n",
    "print(emo_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler_ratio = {\n",
    "    \"neutral\" : 1000,\n",
    "    \"joy\" : 1000,\n",
    "    \"sadness\" : 1000,\n",
    "    \"anger\" : 1000,\n",
    "    \"fear\" : 1000\n",
    "}\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=undersampler_ratio)\n",
    "emo_X, emo_y = rus.fit_resample(emo_cv, emo_data['Emotion'])\n",
    "\n",
    "print(emo_X.shape)\n",
    "print(emo_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emo_data['Emotion'].unique())\n",
    "\n",
    "onehot_label = list()\n",
    "for value in emo_y:\n",
    "    if value == \"neutral\":\n",
    "        onehot_label.append([1, 0, 0, 0, 0])\n",
    "\n",
    "    elif value == \"sadness\":\n",
    "        onehot_label.append([0, 1, 0, 0, 0])\n",
    "\n",
    "    elif value == \"fear\":\n",
    "        onehot_label.append([0, 0, 1, 0, 0])\n",
    "\n",
    "    elif value == \"anger\":\n",
    "        onehot_label.append([0, 0, 0, 1, 0])\n",
    "\n",
    "    elif value == \"joy\":\n",
    "        onehot_label.append([0, 0, 0, 0, 1])\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "onehot_label = np.array(onehot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_X = emo_X.toarray()\n",
    "\n",
    "emo_X_train, emo_X_test, emo_y_train, emo_y_test = train_test_split(emo_X, onehot_label, test_size=.33, random_state=42)\n",
    "print(f'''\n",
    "X_train shape : {emo_X_train.shape}\n",
    "y_train shape : {emo_y_train.shape}\n",
    "\n",
    "X_test shape : {emo_X_test.shape}\n",
    "y_test shape : {emo_y_test.shape}\n",
    "''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepression y onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english') \n",
    "deep_cv = count_vectorizer.fit_transform(data['Clean_TweetText'])\n",
    "deep_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler_ratio = {\n",
    "    0 : 2500,\n",
    "    4 : 2500\n",
    "}\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=undersampler_ratio)\n",
    "deep_X, deep_y = rus.fit_resample(deep_cv, data['target'])\n",
    "\n",
    "print(deep_X.shape)\n",
    "print(deep_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_y_onehot = list()\n",
    "for value in deep_y:\n",
    "    if value == 0:\n",
    "        deep_y_onehot.append([1, 0])\n",
    "\n",
    "    elif value == 4:\n",
    "        deep_y_onehot.append([0, 1])\n",
    "    \n",
    "    else:\n",
    "        break\n",
    "\n",
    "deep_y_onehot = np.array(deep_y_onehot)\n",
    "print(deep_y_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_X = deep_X.toarray()\n",
    "\n",
    "deep_X_train, deep_X_test, deep_y_train, deep_y_test = train_test_split(deep_X, deep_y_onehot, test_size=.33, random_state=42)\n",
    "print(f'''\n",
    "X_train shape : {deep_X_train.shape}\n",
    "y_train shape : {deep_y_train.shape}\n",
    "\n",
    "X_test shape : {deep_X_test.shape}\n",
    "y_test shape : {deep_y_test.shape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_input_layer = layers.Input(shape=(1, 9887), name=\"emotion_model_input\")\n",
    "\n",
    "x1 = layers.Conv1D(4096, 3, padding='same', activation='relu')(emo_input_layer)\n",
    "x1 = layers.Conv1D(2048, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x1)\n",
    "\n",
    "# x1 = layers.Embedding(2048, 1024)(x1)\n",
    "# x1 = layers.GlobalAveragePooling2D()(x1)\n",
    "# x1 = layers.Reshape((1, 1024))(x1)\n",
    "\n",
    "x1 = layers.Conv1D(1024, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.Conv1D(1024, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x1)\n",
    "\n",
    "x1 = layers.Conv1D(512, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.Conv1D(512, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x1)\n",
    "x1 = layers.Dropout(0.5)(x1)\n",
    "\n",
    "x1 = layers.Conv1D(512, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.Conv1D(512, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x1)\n",
    "\n",
    "x1 = layers.Conv1D(256, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.Conv1D(256, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x1)\n",
    "temp_x1 = layers.Flatten()(x1)\n",
    "temp_emo_y = layers.Dense(5, activation='softmax', name=\"Before_GRU_emo\")(temp_x1)\n",
    "\n",
    "x1 = layers.Reshape((1, 256))(x1)\n",
    "x1 = layers.GRU(256)(x1)\n",
    "x1 = layers.Dropout(0.5)(x1)\n",
    "\n",
    "x1 = layers.Flatten()(x1)\n",
    "x1 = layers.Dense(50)(x1)\n",
    "x1 = layers.Dense(30)(x1)\n",
    "x1 = layers.Dense(15)(x1)\n",
    "emo_y = layers.Dense(5, activation='softmax', name=\"final\")(x1)\n",
    "emo_model = Model(inputs=emo_input_layer, outputs=[emo_y, temp_emo_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_input_layer = layers.Input(shape=(1, 546404), name=\"deep_model_input\")\n",
    "# x = layers.Conv1D(2048, 3, padding='same', activation='relu')(deep_input_layer)\n",
    "\n",
    "# x = layers.Conv1D(1024, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.Conv1D(1024, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x)\n",
    "\n",
    "# x = layers.Conv1D(1024, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.Conv1D(1024, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x)\n",
    "\n",
    "# x = layers.Conv1D(512, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.Conv1D(512, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x)\n",
    "\n",
    "# x = layers.Conv1D(512, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.Conv1D(512, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x)\n",
    "\n",
    "# x = layers.Conv1D(256, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.Conv1D(256, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x)\n",
    "\n",
    "# x = layers.Conv1D(256, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.Conv1D(256, 3, padding='same', activation='relu')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x)\n",
    "# temp_deep_y = layers.Dense(2, activation='softmax', name=\"Before_GRU_deep\")(x)\n",
    "\n",
    "# x = layers.Reshape((1, 256))(x)\n",
    "# x = layers.GRU(256)(x)\n",
    "\n",
    "# x = layers.Reshape((4, 64))(x)\n",
    "# x = layers.GRU(64)(x)\n",
    "\n",
    "# x = layers.Dense(50)(x)\n",
    "# x = layers.Dense(30)(x)\n",
    "# x = layers.Dense(15)(x)\n",
    "# deep_y = layers.Dense(2, activation='softmax')(x)\n",
    "# deep_model = Model(inputs=deep_input_layer, outputs=[deep_y, temp_deep_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_model = Model(inputs=[deep_input_layer, emo_input_layer], outputs=[temp_deep_y, deep_y, temp_emo_y, emo_y])\n",
    "# combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam(\n",
    "    lr=0.003\n",
    ")\n",
    "\n",
    "emo_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optim,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "plot_model(emo_model, show_shapes=True, to_file='model_visualization.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "outDir = './cheakpoint/' \n",
    "model_names = outDir + 'weights-{val_final_accuracy:.4f}.h5'\n",
    "def get_callbacks(patience = 50):\n",
    "    model_checkpoint = ModelCheckpoint(model_names, monitor='val_final_accuracy', verbose=1, save_best_only=True, period = 1)\n",
    "    callbacks = [model_checkpoint]\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_X_train = emo_X_train.reshape(-1, 1, 9887)\n",
    "emo_X_test = emo_X_test.reshape(-1, 1, 9887)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_X_test.shape\n",
    "emo_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks()\n",
    "\n",
    "history = emo_model.fit(\n",
    "    emo_X_train, emo_y_train,\n",
    "    shuffle=True,\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    validation_data=(emo_X_test, emo_y_test),\n",
    "    callbacks=[callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model__hist(hist):\n",
    "    path = './cheakpoint/lefms/' # loss, accuracy 그래프 저장할 path\n",
    "\n",
    "    # loss 추이 그래프로 그려서 저장\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.plot(hist.history['loss'], color='b', label=\"Training loss\")\n",
    "    plt.plot(hist.history['val_loss'], color='r', label=\"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # accuracy 추이 그래프로 그려서 저장\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.plot(hist.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "    plt.plot(hist.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "    plt.legend(loc = \"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model__hist(history)\n",
    "loss,acc = emo_model.evaluate(emo_X_test, emo_y_test, verbose=2)\n",
    "print(\"multi_model의 정확도: {:5.2f}%\".format(100*acc))\n",
    "print(\"multi_model의 Loss: {}\".format(loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test (Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesnorflow import keras\n",
    "\n",
    "model_path = './checkpoint/'\n",
    "model_path = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "recon_model = keras.models.load_model(model_path)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbd03b52000256fffc5622fb1d5afa03ae770321afbfaac74e08013d54c137c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
