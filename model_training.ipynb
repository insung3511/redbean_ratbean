{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sending solidarity whoever doctor manage incre...</td>\n",
       "      <td>Stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>need see hair amp beard gat book appointment b...</td>\n",
       "      <td>Anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>next time meet someone new dont ask ask love</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surprise someone love give la senza gift box r...</td>\n",
       "      <td>Lonely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raise hand junhoes ocean lotion life rent free...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets    labels\n",
       "0  sending solidarity whoever doctor manage incre...  Stressed\n",
       "1  need see hair amp beard gat book appointment b...   Anxious\n",
       "2      next time meet someone new dont ask ask love     Normal\n",
       "3  surprise someone love give la senza gift box r...    Lonely\n",
       "4  raise hand junhoes ocean lotion life rent free...    Normal"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/train.csv\",encoding='latin-1')\n",
    "data = data.iloc[:20000]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stressed' 'Anxious' 'Normal' 'Lonely']\n"
     ]
    }
   ],
   "source": [
    "print(data['labels'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = copy.deepcopy(data)\n",
    "\n",
    "stressed_data = data_[data_.labels=='Stressed']\n",
    "anxious_data  = data_[data_.labels=='Anxious']\n",
    "normal_data   = data_[data_.labels=='Normal']\n",
    "lonely_data   = data_[data_.labels=='Lonely']\n",
    "sub_data = pd.concat([stressed_data, anxious_data, normal_data, lonely_data],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target=data.groupby('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Anxious     5502\n",
       "Normal      5355\n",
       "Lonely      4604\n",
       "Stressed    4539\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words\n",
    "\n",
    "Words distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 18296)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english') \n",
    "cv = count_vectorizer.fit_transform(data['tweets'])\n",
    "cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stressed' 'Anxious' 'Normal' 'Lonely']\n"
     ]
    }
   ],
   "source": [
    "print(data['labels'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 4)\n"
     ]
    }
   ],
   "source": [
    "y_label_onehot = list()\n",
    "\n",
    "for value in data['labels']:\n",
    "    if value == \"Stressed\":\n",
    "        y_label_onehot.append([1, 0, 0, 0])\n",
    "\n",
    "    elif value == \"Anxious\":\n",
    "        y_label_onehot.append([0, 1, 0, 0])\n",
    "\n",
    "    elif value == \"Normal\":\n",
    "        y_label_onehot.append([0, 0, 1, 0])\n",
    "\n",
    "    elif value == \"Lonely\":\n",
    "        y_label_onehot.append([0, 0, 0, 1])\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "X = cv.toarray()\n",
    "y_label_onehot = np.array(y_label_onehot)\n",
    "print(y_label_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train shape : (16000, 1, 18296)\n",
      "y_train shape : (16000, 4)\n",
      "\n",
      "X_test shape : (4000, 1, 18296)\n",
      "y_test shape : (4000, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape(-1, 1, 18296)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_label_onehot, test_size=.2, random_state=42)\n",
    "print(f'''\n",
    "X_train shape : {X_train.shape}\n",
    "y_train shape : {y_train.shape}\n",
    "\n",
    "X_test shape : {X_test.shape}\n",
    "y_test shape : {y_test.shape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_input_layer = layers.Input(shape=(1, 18296), name=\"emotion_model_input\")\n",
    "x1 = layers.Conv1D(1024, 3, padding='same', activation='relu')(emo_input_layer)\n",
    "x1 = layers.Conv1D(1024, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x1)\n",
    "\n",
    "x1 = layers.Conv1D(512, 3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01))(x1)\n",
    "x1 = layers.Conv1D(512, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x1)\n",
    "x1 = layers.Dropout(0.5)(x1)\n",
    "temp_x1 = layers.Flatten()(x1)\n",
    "temp_emo_y = layers.Dense(4, activation='softmax', name=\"Before_GRU_emo\")(temp_x1)\n",
    "\n",
    "x1 = layers.Conv1D(512, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.Conv1D(512, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x1)\n",
    "\n",
    "x1 = layers.Conv1D(256, 3, padding='same', activation='relu',  kernel_regularizer=tf.keras.regularizers.L2(0.01))(x1)\n",
    "x1 = layers.Conv1D(256, 3, padding='same', activation='relu')(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.MaxPool1D(pool_size=(2), strides=2, padding='same')(x1)\n",
    "x1 = layers.Reshape((1, 256))(x1)\n",
    "x1 = layers.GRU(256)(x1)\n",
    "x1 = layers.Dropout(0.7)(x1)\n",
    "\n",
    "x1 = layers.Flatten()(x1)\n",
    "x1 = layers.Dense(50)(x1)\n",
    "x1 = layers.Dense(30)(x1)\n",
    "x1 = layers.Dense(15)(x1)\n",
    "emo_y = layers.Dense(4, activation='softmax', name=\"final\")(x1)\n",
    "model = Model(inputs=emo_input_layer, outputs=[emo_y, temp_emo_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " emotion_model_input (InputLaye  [(None, 1, 18296)]  0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 1, 1024)      56206336    ['emotion_model_input[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 1, 1024)      3146752     ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 1, 1024)     4096        ['conv1d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 1, 1024)     0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 1, 512)       1573376     ['max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 1, 512)       786944      ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 1, 512)      2048        ['conv1d_11[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 1, 512)      0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 1, 512)       0           ['max_pooling1d_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 1, 512)       786944      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 1, 512)       786944      ['conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 1, 512)      2048        ['conv1d_13[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 1, 512)      0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 1, 256)       393472      ['max_pooling1d_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 1, 256)       196864      ['conv1d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 1, 256)      1024        ['conv1d_15[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPooling1D)  (None, 1, 256)      0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 256)       0           ['max_pooling1d_7[0][0]']        \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 256)          394752      ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 256)          0           ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 256)          0           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 50)           12850       ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 30)           1530        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 15)           465         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 512)          0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " final (Dense)                  (None, 4)            64          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " Before_GRU_emo (Dense)         (None, 4)            2052        ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 64,298,561\n",
      "Trainable params: 64,293,953\n",
      "Non-trainable params: 4,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.optimizers.Adam(\n",
    "    lr = 0.003\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optim,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "outDir = './cheakpoint/' \n",
    "model_names = outDir + 'weights-{val_final_accuracy:.4f}.h5'\n",
    "def get_callbacks(patience = 50):\n",
    "    model_checkpoint = ModelCheckpoint(model_names, monitor='val_final_accuracy', verbose=1, save_best_only=True, period = 1)\n",
    "    callbacks = [model_checkpoint]\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "callbacks = get_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    shuffle=True,\n",
    "    batch_size=20,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[callbacks]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "101f76880bccf7743e6ad273a1a231f8a0ad778154c99fe069c8ecd7df99a31f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
